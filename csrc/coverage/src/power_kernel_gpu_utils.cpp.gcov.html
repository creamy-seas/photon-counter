<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - üêã Test Coverage - src/power_kernel_gpu_utils.cpp</title>
  <link rel="stylesheet" type="text/css" href="../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../index.html">top level</a> - <a href="index.html">src</a> - power_kernel_gpu_utils.cpp<span style="font-size: 80%;"> (source / <a href="power_kernel_gpu_utils.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">üêã Test Coverage</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">88</td>
            <td class="headerCovTableEntry">95</td>
            <td class="headerCovTableEntryHi">92.6 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2021-05-27 20:33:34</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">5</td>
            <td class="headerCovTableEntry">5</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : /*</a>
<span class="lineNum">       2 </span>            :  * Everything that can be compiled without the nvcc for the GPU power kernel
<span class="lineNum">       3 </span>            :  * - Allocating data on device
<span class="lineNum">       4 </span>            :  * - Fetching kernel parameters
<span class="lineNum">       5 </span>            :  */
<span class="lineNum">       6 </span>            : 
<span class="lineNum">       7 </span>            : #include &lt;cuda_runtime.h&gt; // cudaMalloc cudaFree
<span class="lineNum">       8 </span>            : #include &lt;cuda_runtime_api.h&gt; //for cudaDeviceProp
<span class="lineNum">       9 </span>            : #include &lt;limits.h&gt; // For LONG_MAX
<span class="lineNum">      10 </span>            : #include &lt;string&gt; // For std::to_string
<span class="lineNum">      11 </span>            : 
<span class="lineNum">      12 </span>            : #include &quot;colours.hpp&quot; // RED, OKBLUE etc
<span class="lineNum">      13 </span>            : #include &quot;power_kernel.hpp&quot; // for power kernel parameters
<span class="lineNum">      14 </span>            : #include &quot;ia_ADQAPI.hpp&quot; // for digitiser parameters MAX_DIGITISER_CODE and MAX_NUMBER_OF_RECORDS
<a name="15"><span class="lineNum">      15 </span>            : #include &quot;utils_gpu.hpp&quot; // To fetch GPU parameters</a>
<span class="lineNum">      16 </span>            : 
<span class="lineNum">      17 </span><span class="lineCov">         28 : GPU::PowerKernelParameters::PowerKernelParameters(</span>
<span class="lineNum">      18 </span>            :     int r_points,
<span class="lineNum">      19 </span>            :     int np_points,
<span class="lineNum">      20 </span>            :     int blocks,
<span class="lineNum">      21 </span>            :     int threads_per_block
<span class="lineNum">      22 </span>            :     ){
<span class="lineNum">      23 </span><span class="lineCov">         28 :     this-&gt;r_points = r_points;</span>
<span class="lineNum">      24 </span><span class="lineCov">         28 :     this-&gt;np_points = np_points;</span>
<span class="lineNum">      25 </span><span class="lineCov">         28 :     this-&gt;blocks = blocks;</span>
<span class="lineNum">      26 </span><span class="lineCov">         28 :     this-&gt;threads_per_block = threads_per_block;</span>
<span class="lineNum">      27 </span>            : 
<span class="lineNum">      28 </span><span class="lineCov">         28 :     this-&gt;print();</span>
<a name="29"><span class="lineNum">      29 </span><span class="lineCov">         28 : }</span></a>
<span class="lineNum">      30 </span>            : 
<span class="lineNum">      31 </span><span class="lineCov">         28 : void GPU::PowerKernelParameters::print(){</span>
<span class="lineNum">      32 </span><span class="lineCov">         28 :     OKBLUE(&quot;===========================================&quot;);</span>
<span class="lineNum">      33 </span><span class="lineCov">         28 :     RED(&quot;          **POWER KERNEL**&quot;);</span>
<span class="lineNum">      34 </span>            : 
<span class="lineNum">      35 </span><span class="lineCov">         28 :     OKBLUE(&quot;Data Parameters&quot;);</span>
<span class="lineNum">      36 </span><span class="lineCov">         28 :     WHITE(&quot;R_POINTS: %i\n&quot;, this-&gt;r_points );</span>
<span class="lineNum">      37 </span><span class="lineCov">         28 :     WHITE(&quot;SP_POINTS: %i\n&quot;, this-&gt;np_points );</span>
<span class="lineNum">      38 </span>            : 
<span class="lineNum">      39 </span><span class="lineCov">         28 :     OKBLUE(&quot;Processing Parameters&quot;);</span>
<span class="lineNum">      40 </span><span class="lineCov">         28 :     WHITE(&quot;BLOCKS: %i\n&quot;, this-&gt;blocks );</span>
<span class="lineNum">      41 </span><span class="lineCov">         28 :     WHITE(&quot;THREADS_PER_BLOCK: %i\n&quot;, this-&gt;threads_per_block );</span>
<span class="lineNum">      42 </span>            : 
<span class="lineNum">      43 </span><span class="lineCov">         28 :     OKBLUE(&quot;===========================================&quot;);</span>
<a name="44"><span class="lineNum">      44 </span><span class="lineCov">         28 : }</span></a>
<span class="lineNum">      45 </span>            : 
<span class="lineNum">      46 </span><span class="lineCov">         28 : GPU::PowerKernelParameters GPU::fetch_kernel_parameters(){</span>
<span class="lineNum">      47 </span>            :     // Even number required for summation on GPU
<span class="lineNum">      48 </span>            :     if (R_POINTS % 2 != 0)
<span class="lineNum">      49 </span>            :         throw std::runtime_error(
<span class="lineNum">      50 </span>            :             &quot;R_POINTS=&quot;
<span class="lineNum">      51 </span>            :             + std::to_string(R_POINTS)
<span class="lineNum">      52 </span>            :             + &quot; needs to be a even number.&quot;);
<span class="lineNum">      53 </span>            : 
<span class="lineNum">      54 </span>            :     // Check that &quot;chunking&quot; falls within the limits of shared memory on GPU
<span class="lineNum">      55 </span><span class="lineCov">         28 :     cudaDeviceProp prop = fetch_gpu_parameters();</span>
<span class="lineNum">      56 </span><span class="lineCov">         28 :     const int number_of_cumulative_arrays = 4;</span>
<span class="lineNum">      57 </span>            :     const int shared_memory_required = (R_POINTS_PER_CHUNK
<span class="lineNum">      58 </span>            :                                         * sizeof(long)
<span class="lineNum">      59 </span><span class="lineCov">         28 :                                         * number_of_cumulative_arrays);</span>
<span class="lineNum">      60 </span><span class="lineCov">         28 :     if (prop.sharedMemPerBlock &lt; shared_memory_required)</span>
<span class="lineNum">      61 </span>            :         throw std::runtime_error(
<span class="lineNum">      62 </span>            :             &quot;Not enough shared memory on GPU (&quot;
<span class="lineNum">      63 </span><span class="lineNoCov">          0 :             + std::to_string(shared_memory_required)</span>
<span class="lineNum">      64 </span><span class="lineNoCov">          0 :             + &quot; &gt; &quot;</span>
<span class="lineNum">      65 </span><span class="lineNoCov">          0 :             + std::to_string(prop.sharedMemPerBlock)</span>
<span class="lineNum">      66 </span><span class="lineNoCov">          0 :             + &quot; bytes) for using R_POINTS_PER_CHUNK=&quot;</span>
<span class="lineNum">      67 </span><span class="lineNoCov">          0 :             + std::to_string(R_POINTS_PER_CHUNK)</span>
<span class="lineNum">      68 </span><span class="lineNoCov">          0 :             + &quot; in power mesurements.&quot;</span>
<span class="lineNum">      69 </span><span class="lineNoCov">          0 :             );</span>
<span class="lineNum">      70 </span>            : 
<span class="lineNum">      71 </span>            :     // Check that chunking of R_POINTS is valid
<span class="lineNum">      72 </span>            :     if (R_POINTS &lt; R_POINTS_PER_CHUNK)
<span class="lineNum">      73 </span>            :         throw std::runtime_error(
<span class="lineNum">      74 </span>            :             &quot;R_POINTS (&quot;
<span class="lineNum">      75 </span>            :             + std::to_string(R_POINTS)
<span class="lineNum">      76 </span>            :             + &quot;) &lt; R_POINTS_PER_CHUNK (&quot;
<span class="lineNum">      77 </span>            :             + std::to_string(R_POINTS_PER_CHUNK)
<span class="lineNum">      78 </span>            :             + &quot;): Chunking is bigger than amount of repititions on digitiser.&quot;
<span class="lineNum">      79 </span>            :             );
<span class="lineNum">      80 </span>            :     if ((R_POINTS - (R_POINTS / R_POINTS_PER_CHUNK) * R_POINTS_PER_CHUNK) != 0)
<span class="lineNum">      81 </span>            :         throw std::runtime_error(
<span class="lineNum">      82 </span>            :             &quot;R_POINTS_PER_CHUNK (&quot;
<span class="lineNum">      83 </span>            :             + std::to_string(R_POINTS_PER_CHUNK)
<span class="lineNum">      84 </span>            :             + &quot;) does not fit fully into R_POINTS (&quot;
<span class="lineNum">      85 </span>            :             + std::to_string(R_POINTS)
<span class="lineNum">      86 </span>            :             + &quot;).&quot;);
<span class="lineNum">      87 </span>            :     if ((R_POINTS / R_POINTS_PER_CHUNK % 2) != 0)
<span class="lineNum">      88 </span>            :             throw std::runtime_error(
<span class="lineNum">      89 </span>            :                 &quot;R_POINTS_PER_CHUNK (&quot;
<span class="lineNum">      90 </span>            :                 + std::to_string(R_POINTS_PER_CHUNK)
<span class="lineNum">      91 </span>            :                 + &quot;) does not chunk R_POINTS (&quot;
<span class="lineNum">      92 </span>            :                 + std::to_string(R_POINTS)
<span class="lineNum">      93 </span>            :                 + &quot;) evenly across the 2 streams.&quot;);
<span class="lineNum">      94 </span>            : 
<span class="lineNum">      95 </span>            :     // Ensure that the cumulative arrays will not overflow.
<span class="lineNum">      96 </span>            :     if ((MAX_DIGITISER_CODE * MAX_NUMBER_OF_RECORDS &gt; LONG_MAX) ||
<span class="lineNum">      97 </span>            :         (MAX_DIGITISER_CODE * MAX_DIGITISER_CODE * MAX_NUMBER_OF_RECORDS &gt; LONG_MAX) ||
<span class="lineNum">      98 </span>            :         (2 * MAX_DIGITISER_CODE * MAX_DIGITISER_CODE * MAX_NUMBER_OF_RECORDS &gt; LONG_MAX))
<span class="lineNum">      99 </span>            :         throw std::runtime_error(
<span class="lineNum">     100 </span>            :             &quot;Cumulative arrays will not be able to hold all the intermediate processing data for power measurements&quot;);
<span class="lineNum">     101 </span>            : 
<span class="lineNum">     102 </span>            :     // Reset is checked in python
<span class="lineNum">     103 </span>            :     return GPU::PowerKernelParameters(
<span class="lineNum">     104 </span>            :         R_POINTS,
<span class="lineNum">     105 </span>            :         SP_POINTS,
<span class="lineNum">     106 </span>            :         BLOCKS,
<span class="lineNum">     107 </span>            :         THREADS_PER_BLOCK
<span class="lineNum">     108 </span><span class="lineCov">         28 :         );</span>
<a name="109"><span class="lineNum">     109 </span>            : }</a>
<span class="lineNum">     110 </span>            : 
<span class="lineNum">     111 </span><span class="lineCov">         43 : void GPU::allocate_memory(</span>
<span class="lineNum">     112 </span>            :     short **chA_data, short **chB_data,
<span class="lineNum">     113 </span>            :     short ****gpu_in, long****gpu_out, long ****cpu_out, int no_streams){
<span class="lineNum">     114 </span>            :     /** There is a lot of derefenecing in this function, since the arrays ara passed in by address &amp; */
<span class="lineNum">     115 </span>            : 
<span class="lineNum">     116 </span><span class="lineCov">         43 :     OKBLUE(&quot;Power Kernel: Allocating memory on GPU and CPU.&quot;);</span>
<span class="lineNum">     117 </span><span class="lineCov">         43 :     int chunks = R_POINTS / R_POINTS_PER_CHUNK;</span>
<span class="lineNum">     118 </span><span class="lineCov">         43 :     if (chunks - (chunks / no_streams) * no_streams != 0)</span>
<span class="lineNum">     119 </span>            :         throw std::runtime_error(
<span class="lineNum">     120 </span>            :             &quot;Power Kernel: no_streams (&quot;
<span class="lineNum">     121 </span><span class="lineCov">         14 :             + std::to_string(no_streams)</span>
<span class="lineNum">     122 </span><span class="lineCov">         14 :             + &quot;) does no fit fully into R_POINTS/R_POINTS_PER_CHUNK (&quot;</span>
<span class="lineNum">     123 </span><span class="lineCov">         28 :             + std::to_string(R_POINTS) + &quot;/&quot; + std::to_string(R_POINTS_PER_CHUNK)</span>
<span class="lineNum">     124 </span><span class="lineCov">         21 :             + &quot;) = &quot; + std::to_string(chunks));</span>
<span class="lineNum">     125 </span>            : 
<span class="lineNum">     126 </span><span class="lineCov">         36 :     int success = 0; int odx;</span>
<span class="lineNum">     127 </span>            : 
<span class="lineNum">     128 </span>            :     // Digitiser will transfer data into memory-locked arrays, made with cudaHostAlloc
<span class="lineNum">     129 </span><span class="lineCov">         36 :     if (chA_data != 0)</span>
<span class="lineNum">     130 </span>            :         success += cudaHostAlloc((void**)chA_data,
<span class="lineNum">     131 </span>            :                                  SP_POINTS * R_POINTS * sizeof(short),
<span class="lineNum">     132 </span><span class="lineCov">         36 :                                  cudaHostAllocDefault);</span>
<span class="lineNum">     133 </span><span class="lineCov">         36 :     if (chB_data != 0)</span>
<span class="lineNum">     134 </span>            :         success += cudaHostAlloc((void**)chB_data,
<span class="lineNum">     135 </span>            :                                  SP_POINTS * R_POINTS * sizeof(short),
<span class="lineNum">     136 </span><span class="lineCov">         36 :                                  cudaHostAllocDefault);</span>
<span class="lineNum">     137 </span><span class="lineCov">         36 :     if (success != 0) FAIL(&quot;Power Kernel: Failed to allocate locked input memory on CPU.&quot;);</span>
<span class="lineNum">     138 </span>            : 
<span class="lineNum">     139 </span>            : 
<span class="lineNum">     140 </span>            :     // Input data is fed in chunks of R_POINTS_PER_CHUNK * SP_POINTS
<span class="lineNum">     141 </span>            :     // (gpu_in is passed in by address, so dereference first, and assign it to a new 2D array of stream x channels)
<span class="lineNum">     142 </span><span class="lineCov">         36 :     if (gpu_in != 0) {</span>
<span class="lineNum">     143 </span><span class="lineCov">         32 :         (*gpu_in) = new short**[no_streams];</span>
<span class="lineNum">     144 </span><span class="lineCov">        103 :         for (int s(0); s &lt; no_streams; s++){</span>
<span class="lineNum">     145 </span>            :             // (each stream will have entries for chA and chB)
<span class="lineNum">     146 </span><span class="lineCov">         71 :             (*gpu_in)[s] = new short*[2];</span>
<span class="lineNum">     147 </span>            : 
<span class="lineNum">     148 </span>            :             // (chA and chB will store the address (short*) of the memory allocated on GPU)
<span class="lineNum">     149 </span>            :             // (- they need to be passed in by address &amp; in order for cudaMalloc to update their values)
<span class="lineNum">     150 </span><span class="lineCov">         71 :             success += cudaMalloc((void**)&amp;(*gpu_in)[s][CHA], SP_POINTS * R_POINTS_PER_CHUNK * sizeof(short));</span>
<span class="lineNum">     151 </span><span class="lineCov">         71 :             success += cudaMalloc((void**)&amp;(*gpu_in)[s][CHB], SP_POINTS * R_POINTS_PER_CHUNK * sizeof(short));</span>
<span class="lineNum">     152 </span>            :         }
<span class="lineNum">     153 </span>            :     }
<span class="lineNum">     154 </span><span class="lineCov">         36 :     if (success != 0) FAIL(&quot;Power Kernel: Failed to allocate input memory on GPU.&quot;);</span>
<span class="lineNum">     155 </span>            : 
<span class="lineNum">     156 </span><span class="lineCov">         36 :     if (gpu_out != 0 &amp;&amp; cpu_out != 0) {</span>
<span class="lineNum">     157 </span>            :         // Each stream will have it's dedicated arrays for writting results to
<span class="lineNum">     158 </span><span class="lineCov">         32 :         (*gpu_out) = new long**[no_streams];</span>
<span class="lineNum">     159 </span><span class="lineCov">         32 :         (*cpu_out) = new long**[no_streams];</span>
<span class="lineNum">     160 </span><span class="lineCov">        103 :         for (int s(0); s &lt; no_streams; s++) {</span>
<span class="lineNum">     161 </span><span class="lineCov">         71 :             (*gpu_out)[s] = new long*[GPU::no_outputs_from_gpu];</span>
<span class="lineNum">     162 </span><span class="lineCov">         71 :             (*cpu_out)[s] = new long*[GPU::no_outputs_from_gpu];</span>
<span class="lineNum">     163 </span><span class="lineCov">        355 :             for (int i(0); i &lt; GPU::no_outputs_from_gpu; i++) {</span>
<span class="lineNum">     164 </span><span class="lineCov">        284 :                 odx = GPU::outputs_from_gpu[i];</span>
<span class="lineNum">     165 </span>            : 
<span class="lineNum">     166 </span>            :                 // Processed Output data will accumulate on the GPU for each stream
<span class="lineNum">     167 </span><span class="lineCov">        284 :                 success += cudaMalloc((void**)&amp;(*gpu_out)[s][odx], SP_POINTS * sizeof(long));</span>
<span class="lineNum">     168 </span><span class="lineCov">        284 :                 if (success != 0) FAIL(&quot;Power Kernel: Failed to allocate output memory on GPU.&quot;);</span>
<span class="lineNum">     169 </span>            : 
<span class="lineNum">     170 </span>            :                 // And will be copied and summed on the cpu
<span class="lineNum">     171 </span><span class="lineCov">        284 :                 success += cudaHostAlloc((void**)&amp;(*cpu_out)[s][odx], SP_POINTS * sizeof(long), cudaHostAllocDefault);</span>
<span class="lineNum">     172 </span><span class="lineCov">        284 :                 if (success != 0) FAIL(&quot;Power Kernel: Failed to allocate locked output memory on CPU.&quot;);</span>
<span class="lineNum">     173 </span>            :             }
<span class="lineNum">     174 </span>            :         }
<span class="lineNum">     175 </span>            :     }
<span class="lineNum">     176 </span>            : 
<span class="lineNum">     177 </span><span class="lineCov">         36 :     OKGREEN(&quot;Power Kernel: Allocation done!&quot;);</span>
<a name="178"><span class="lineNum">     178 </span><span class="lineCov">         36 : }</span></a>
<span class="lineNum">     179 </span>            : 
<span class="lineNum">     180 </span><span class="lineCov">         36 : void GPU::free_memory(</span>
<span class="lineNum">     181 </span>            :     short *chA_data, short *chB_data,
<span class="lineNum">     182 </span>            :     short ***gpu_in, long ***gpu_out, long ***cpu_out, int no_streams){
<span class="lineNum">     183 </span>            : 
<span class="lineNum">     184 </span><span class="lineCov">         36 :     OKBLUE(&quot;Power Kernel: Deallocating memory on GPU and CPU.&quot;);</span>
<span class="lineNum">     185 </span><span class="lineCov">         36 :     int success = 0; int odx;</span>
<span class="lineNum">     186 </span>            : 
<span class="lineNum">     187 </span><span class="lineCov">         36 :     if (chA_data != 0)</span>
<span class="lineNum">     188 </span><span class="lineCov">         36 :         success += cudaFreeHost(chA_data);</span>
<span class="lineNum">     189 </span><span class="lineCov">         36 :     if (chB_data != 0)</span>
<span class="lineNum">     190 </span><span class="lineCov">         36 :         success += cudaFreeHost(chB_data);</span>
<span class="lineNum">     191 </span><span class="lineCov">         36 :     if (success != 0) FAIL(&quot;Power Kernel: Failed to free locked input memory on CPU.&quot;);</span>
<span class="lineNum">     192 </span>            : 
<span class="lineNum">     193 </span><span class="lineCov">         36 :     if (gpu_in != 0) {</span>
<span class="lineNum">     194 </span><span class="lineCov">        103 :         for (int s(0); s &lt; no_streams; s++) {</span>
<span class="lineNum">     195 </span><span class="lineCov">         71 :             success += cudaFree(gpu_in[s][CHA]);</span>
<span class="lineNum">     196 </span><span class="lineCov">         71 :             success += cudaFree(gpu_in[s][CHB]);</span>
<span class="lineNum">     197 </span><span class="lineCov">         71 :             delete[] gpu_in[s];</span>
<span class="lineNum">     198 </span>            :         }
<span class="lineNum">     199 </span><span class="lineCov">         32 :         delete[] gpu_in;</span>
<span class="lineNum">     200 </span>            :     }
<span class="lineNum">     201 </span><span class="lineCov">         36 :     if (success != 0) FAIL(&quot;Power Kernel: Failed to free input memory on GPU.&quot;);</span>
<span class="lineNum">     202 </span>            : 
<span class="lineNum">     203 </span><span class="lineCov">         36 :     if (gpu_out != 0 &amp;&amp; cpu_out != 0) {</span>
<span class="lineNum">     204 </span><span class="lineCov">        103 :         for (int s(0); s &lt; no_streams; s++) {</span>
<span class="lineNum">     205 </span><span class="lineCov">        355 :             for (int i(0); i &lt; GPU::no_outputs_from_gpu; i++) {</span>
<span class="lineNum">     206 </span><span class="lineCov">        284 :                 odx = GPU::outputs_from_gpu[i];</span>
<span class="lineNum">     207 </span>            : 
<span class="lineNum">     208 </span><span class="lineCov">        284 :                 success += cudaFree(gpu_out[s][odx]);</span>
<span class="lineNum">     209 </span><span class="lineCov">        284 :                 if (success != 0) FAIL(&quot;Power Kernel: Failed to free output memory on GPU.&quot;);</span>
<span class="lineNum">     210 </span>            : 
<span class="lineNum">     211 </span><span class="lineCov">        284 :                 success += cudaFreeHost(cpu_out[s][odx]);</span>
<span class="lineNum">     212 </span><span class="lineCov">        284 :                 if (success != 0) FAIL(&quot;Power Kernel: Failed to free locked outputa memory on CPU.&quot;);</span>
<span class="lineNum">     213 </span>            :             }
<span class="lineNum">     214 </span><span class="lineCov">         71 :             delete[] gpu_out[s];</span>
<span class="lineNum">     215 </span><span class="lineCov">         71 :             delete[] cpu_out[s];</span>
<span class="lineNum">     216 </span>            :         }
<span class="lineNum">     217 </span><span class="lineCov">         32 :         delete[] gpu_out;</span>
<span class="lineNum">     218 </span><span class="lineCov">         32 :         delete[] cpu_out;</span>
<span class="lineNum">     219 </span>            :     }
<span class="lineNum">     220 </span>            : 
<span class="lineNum">     221 </span><span class="lineCov">         36 :     OKGREEN(&quot;Power Kernel: Memory freed!&quot;);</span>
<span class="lineNum">     222 </span><span class="lineCov">         36 : }</span>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="http://ltp.sourceforge.net/coverage/lcov.php" target="_parent">LCOV version 1.13</a></td></tr>
  </table>
  <br>

</body>
</html>
